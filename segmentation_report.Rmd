---
title: "Form perception in classical music: Listeners ratings vs. music theoretical predictions"
author: Klaus Frieler
date: 23.08.2023
output: word_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", echo = FALSE, warning = FALSE, message = FALSE)
options(tidyverse.quite = T)
library(tidyverse)
library(flextable)
library(ggpubr)
source("analysis.R")
source("workspace_analysis.R")
source("plot_util.R")
setup_workspace()
piece_names <- c("Louise Farrenc, Nonet, 1st mov", "John K. Paine, Symphony No. 1, 1st mov")
piece_names_short <- c("Louise Farrenc", "John K. Paine")
#seg_stats_new3 <- get_segmentation_stats(ground_truth = ground_truth %>% filter(level < 3), band_widths = seq(0.5, 4, .25))

```

# Method
In an online and a lab experiment, participant were asked to mark beginning or endings  in two pieces of classical music while listening to the piece. These markers delineate sections or segments, and a set of markers is called a *segmentation* of the musical piece. In the lab version, participants  were asked further questions on the nature of their segmentation marks, which are not considered here. 

There were a total of `r n_distinct(metadata$p_id)` participants (`r as.numeric(table(metadata$gender)["female"])` female), with `r n_distinct(metadata[metadata$source == "lab",]$p_id)` in the lab experiment.  and `r n_distinct(metadata[metadata$source == "online",]$p_id)` in the online version. Mean age was `r round(mean(metadata$age, na.rm = T), 1)` (SD = `r round(sd(metadata$age, na.rm = T),1)`). Mean GoldMSI values was `r round(mean(metadata$GMS.general, na.rm = T), 1)` (SD = `r round(sd(metadata$GMS.general, na.rm = T), 1)`), which is about the German averages (Schaal et al.)

We excluded  `r n_distinct(metadata$p_id) - n_distinct(all_online$p_id)`  participants from the online version due to missing data (e.g., no segments were marked).


## Analysis
### General observations

Our main data are time positions, measured in seconds from the start of a piece. In the lab experiment, the participants completed two trials for each piece, whereas in the online version, there was only one trial. Additionally, we analyzed the pieces with respect to endings and beginnings based on music theory, using two different levels of granularity.

```{r piece_stats}
get_boundary_stats() %>% 
  group_by(piece, source, trial, level) %>% 
  summarise(n_p = n_distinct(p_id), 
            n = n_distinct(time_in_s),
            mean_isi = round(mean(m), 1),
            sd_isi =  round(mean(s, na.rm =T), 1),
            .groups = "drop") %>% 
  arrange(piece, source, trial) %>%
  mutate(piece = piece_names_short[piece] %>% remove_doublets(), 
         source = remove_doublets(source) 
         #level = remove_doublets(level),
         #trial = remove_doublets(trial)
         ) %>% 
  flextable() %>% 
  set_header_labels(values = list(piece = "Piece", 
                                  source = "Source", 
                                  trial = "Trial", 
                                  level = "Level", 
                                  n_p = "No. Part.", 
                                  n = "No. Markers",
                                  mean_isi = "Mean ISI (s)",
                                  sd_isi = "SD ISI (s)")) %>% 
  set_caption("Tab. 1. Overview of time marker data.") %>% 
  add_footer_lines("Note: ISI = inter segment interval.") %>% 
  fontsize(i = NULL, j = NULL, size = 8, part = "all") %>% 
  autofit()
```

Notably, the level 2 segments from the music theoretical analysis have much smaller ISIs (intersegment interval) than all other subsets. The distribution of ISIs can be found in Fig. 1, where we used the logarithm of ISI, to make the distribution more close to a normal distribution, as the ISIs follow roughly a log-normal distribution. This also holds true for the theoretical segment, which is an interesting fact in itself.


```{r isi_dist, fig.cap = "Figure 1. Distribitution of intersegment intervals.", fig.width = 12, fig.height = 8, include = T, warning = F}
plot_isi_dist() + labs(subtitle = "Dotted lines are theory means")

```

Fig. 1 further suggest that in the lab condition, the mean log ISIS might be bi-modally distributed. However, fitting a Gaussian mixture model using the ``Mclust`` package and automatic cluster number selection did not corroborate this.

We checked whether the participant log ISIs are different between pieces and trials by using a linear mixed model with  participants as random effects and interactions of piece and trial and of piece and source as fixed effects. The results can be seen in Tab. 2. Indeed, all coefficients became significant. The second piece had generally longer ISIs than the first piece. Similarly, second trials had longer ISIs as well, though the increase is a bit smaller for the second piece here. Finally, in the online experiment the participants had slightly but significantly smaller ISIs for the first piece (`r piece_names[1]`). 

```{r lm_piece_trial}
mod <- all_boundaries %>% 
  mutate(trial = factor(trial), piece = factor(piece)) %>% 
  group_by(p_id, trial, piece, source) %>% 
  
  summarise(m = log(mean(diff(time_in_s))), .groups = "drop") %>% 
  lmerTest::lmer(log(m) ~ 0 + piece * (trial + source) + (1|p_id), data = .) 

mod %>% broom.mixed::tidy() %>% 
  mutate(across(where(is.numeric), function(.x) round(.x, 3) )) %>%
  flextable() %>% 
  fontsize(i = NULL, j = NULL, size = 8, part = "all") %>% 
  autofit() %>% 
  set_caption("Tab. 2. Linear mixed model of log ISI for piece and trial")
```

### Interrater agreement and agreement with theory
A general problem here is the very fine grained resolution of 1 ms of the empirical markers. Reaction times of humans are typically in the order of 500 ms, whereas intersegment intervals (ISI) are in the order of several 10 seconds. This lends itself to use some form of binning or other bandwidth-based technique when comparing one of more segmentations. This introduces a principally free parameter (bin size or bandwidth(), for which not a priori value can be given, though it should be in the range of 1 second. 

An alternative, parameter-free technique is dynamic time warping, which creates an (optimal) alignment between two series of time points. However, for aggregating all participants segmentations in a single "collective" or average segmentation, a bandwidth parameter is still needed.

Another issue is that segmentations are generally sparse in nature, typically in the order of 10 segments (mean number of segments = `r round(all_boundaries %>% group_by(p_id, piece, trial, source) %>% summarise(n = n(), .groups = "drop") %>% summarise(m_n = mean(n)) %>% pull(m_n), 1)`) over a set of several 100.000  possible  points (based on a millisecond resolution), Even after binning with a bandwidth of several seconds, this is still a problem. Hence, any metrics that counts true negatives is not well-suited as it will over-estimate the agreement considerably. A further problem is to establish a suitable baseline to check whether the agreement is significantly different from random segmentations.

### Gaussification
One method to aggregate distinct sets of time points is called "Gaussification" (Frieler, 2004), which is simply a weighted sum of of Gaussians with the time points as means and a free bandwidth parameter as standard deviations. If the weights are all equal this is equivalent to a Gaussian kernel density estimation (up to constant factor). In Fig. 2, two gaussifications with bandwidth 2 s overlaid with all segmentation marks are depicted. The density of the segmentations marks correspond with the peaks of gaussification.

```{r gaussifications, fig.cap = "Figure 2. Gaussification (red line) and segmentation marks (blue-gray lines) for piece 1 (top) and piece 2 (bottom).", fig.width = 12, fig.height = 12, include = T}
p <- plot_gaussification(all_boundaries %>% filter(piece == 1), 
                         end = piece_durations[1], 
                         external_markers = all_boundaries %>% filter(piece == 1),
                         segment_data = ground_truth %>% filter(piece == 1, level <= 2, theory == 1)) + labs(title = piece_names[1])
q <- plot_gaussification(all_boundaries %>% filter(piece == 2), 
                         end = piece_durations[2], 
                         external_markers = all_boundaries %>% filter(piece == 2),
                         segment_data = ground_truth %>% filter(piece == 2, level <= 2, theory == 1)) + labs(title = piece_names[2])
ggarrange(p,q, ncol = 1)
```

A gaussification (or- probability density) allows to define aggregate segmentations by its peaks (maxima). Here, a threshold, another free parameter,  can be used to filter the peaks further, e.g., by keeping only the peaks with heights larger the mean or the median of all peak heights. This allows to keep only points of largest agreement. 

Peak heights is corresponds approximately to the number of participants' marks contributing to this peak.  If *N*  participants would have completely identical sets of segmentation marks, then this statement is exactly true, as one would add *N* (unnormalized) Gaussians over the same mean value in this case. The peak positions can also be regarded as the local mean position of segmentation markers. For a single set of segmentation marks, the peaks of its gaussification reproduce obviously the segmentation marks themselves.

### Dynamic Time Warping
Dynamic time warping is a very powerful method to align two sets of points in a optimal (i.e. minimal) way based on a cost function. The distances between the aligned points can then be used as an indicator of similarity of the sets of time points. We used the ``dtw`` package for R, which returns a normalized distance value which we used for comparison. This value is depending on absolute offsets and time shifts, but all time points are fixed between the start (0) and the end of the pieces (322 vs. 421 s), so this is not an issue.

```{r dtw_example, fig.cap = "Figure 3. Sample DTW alignment between a participant (query, red dots) and the level 1 ground truth segments (reference, blue dots) for piece 1. Arrows indicate the mapping of points, the distance is the sum of horizontal shifts by this arrows.", fig.width = 12, fig.height = 8, include = T}

plot_dtw_alignment(all_boundaries %>% 
                     filter(p_id == p_id[1], piece == 1, trial == 1) %>% 
                     pull(time_in_s), 
                   ground_truth %>% 
                     filter(piece == 1, level == 1) %>% 
                     pull(time_in_s)) + theme(axis.text.y = element_text(size = 12), legend.position = "none")
```

### F1 scores
The F1 score is defined as the harmonic mean of specificity (to find only correct instances) and sensitivity (to find all correct instances)  of  information retrieval system. This can be translated into a measure of similarity of two binary sequences, where 1 indicate positive instance (the presence of something) and 0 indicate negative instances (the absence of something). 

If one has two binary sequences, one called the query and the other the reference, then it can expressed as the ratio of true positives to the sum of true positives and the arithmetic mean of false positives and false negatives. A true positive is a location where two 1's coincide. A false positive is a location where a one the query matches a zero in the reference. A false negative is a location where a zero in the query matches a one in the reference. (A true negative is a location where two zeros match, but this is not considered in the F1 score). As false positives and false negatives are counted for in the same fashion, they are interchangeable and thus the role of query and reference can be exchanged without altering the F1 score. Thus, only location where at least one one is present is counted for the F1 scores, one could add as many matching zero pairs as one wants without changing the F1 score. 

To use F1-scores as similarity measure for a pair of segmentations, the underlying time span needs to be identical and and  segmented into identical bins, i.e. time segments that cover the full range . The bins  can be overlapping or disjoint, but they have to match. For each bin, a one is recorded if at least one segmentation mark is inside its boundary, or a zero, otherwise. 

#### Example: F1 scores
One segmentation over the time span of 60 s has time points 10, 20, 30, 40, 50, whereas another one has 15, 25, 35, 45, 55. Covering the time span with three disjoint (non-overlapping) bins of 20 sec each (including the left and excluding the right boundary), translates the first segmentation into ``1, 1, 1`` and the second into ``0, 1, 1``. The number of true positives is two, the number of false positives (or false negatives) is one. This results in a F1 score of 
``F1 = 2/(2 + .5 * (1 + 0)) = 2/2.5 = .8)``. 

If we would have used  non-overlapping bins of five second duration, the first sequence would be have been translated to ``0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0`` an the second to ``0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1``. This a complete different scenario, as there are no true positives now, so the F1 scores here would be zero. 
This illustrates quite clearly the influence of bin size and whether the bins are overlapping or not. In order to stabilize F1 score, we will (logically) add two binary sequences resulting from the same binning but with half a bin of overlap. This allows to be


### Interrater Agreement
The heights of the peaks vs. the height of the troughs of a Gaussification of set of segmentations can be regarded as a measure of inter-rater agreement. If there is substantial agreement, segmentation marks from different raters are close to each other resulting in higher peaks and lower troughs. If there is complete agreement (with a suitable small bandwidth) between *N* raters, there will be peaks of height *N* and troughs with zero height. Based on this, we can define an inter-rater agreement metric as the difference between mean peak and mean trough height, divided by the number of raters. This yields a number between zero and one, where one means perfect agreement. However, as the heights of the peaks and the troughs are correlated by definition of the gaussification, we can also use a simpler measure but dividing the mean peak height by the number of raters.

## Hypothesis testing
In order to calibrate our scores of inter-rater agreement and agreement with theory, we need to find a baseline as there are no standard hypothesis tests that cover our very specific measures. To this end, we use simulations to compare the real agreements with agreements of random segmentations. Simulation are either based on the ground truth data or the participants' segmentations. For this, we use the observation (see above) that the inter-segmentation intervals follow roughly a log-normal distribution. This can be used to sample a set of log ISIs from a a normal distribution and converting them into to a segmentation by cumulative summing the exponentiated intervals, starting, but not including zero (as zero is the trivial segment border). Having these simulations, we can check whether the real metrics is significantly different from pure chance values. 


# Results
## Inter-rater agreement
We employed the full method for inter-rater agreement (based on peaks and troughs) for the two pieces, for both  trials separately. We compared the values to 50 simulation each, based on the participants' segmentations of the corresponding case. We were using just a single bandwidth of ``2 s`` due to the long calculation need for the simulations (and quick experimentation showed that the parameter seems to have little influence on the results).

Results can be found in Tab. 3. The agreement values range between .12 and .17, and are thus not very strong, which is not unexpected giving the enormous number of degrees of freedom, but are clearly significantly different from random agreement which ranges from .025 to .041 in the simulations. Further observations are that the second piece has slightly larger agreement than the second piece and that agreement in the second trials is larger than in the first trials. 

```{r interrater_agreements}
readRDS("data/peakiness_irr_s2.rds") %>% 
  select(Piece = piece, Trial = trial,`N Raters`= n_rater, `Agreement Raters` = mu,  `Mean Agreement (Simulations)` = estimate,  `t` = statistic, `p` = p.value) %>% 
  mutate(across(where(is.numeric), function(.x) round(.x, 3) )) %>%
  flextable() %>% 
  fontsize(i = NULL, j = NULL, size = 10, part = "all") %>% 
  autofit() %>% 
  set_caption("Tab. 3. Interrater agreement of trials (lab and online) vs. random agreement by simulation of N = 50 runs with sigmas of 2 s")
```

## Agreement with theory
We used  normalized DTW distance and F1 scores as metrics to compare participants' segmentations with theoretical predictions. We compared two levels of theoretical granularity over a range of bandwidth values from *.5 s* to *4 s* in steps of *100 ms*. For each bandwidth and theoretical level, we first aggregated the participants' rating using gaussification and a subsequent downsampling by selecting  all peaks with values exceeding the mean of all peaks. This yields a summary segmentation for which F1 scores and normalized DTW distance (as described above) with the theoretical predictions can be determined. Additionally, we calculated the same values for  *N = 25*  random  segmentations based on the summary segmentation and the bandwidth. The results can be found in Fig. 4a and 4b.


```{r band_with_f1, fig.cap = "Figure 4a. F1 scores of summary (red) and random (blue) segmentation agains theoretical predictions over band width for pieces and granularity levels of theory. ", fig.width = 12, fig.height = 8, include = T}
plot_bandwidth(seg_stats, type = "F1")

```


```{r band_with_nd, fig.cap = "Figure 4b. DTW normalized of summary (red) and random (blue) segmentation against theoretical predictions over band width for pieces and granularity levels of theory.", fig.width = 12, fig.height = 8, include = T}
plot_bandwidth(seg_stats, type = "normalized distance")

```

It is clear, that the agreement of the theoretical prediction with the summary segmentation is significantly better for most parts of the bandwidth range for both metrics (note that for normalized distance larger values mean lower agreement), for both pieces and the coarsest granularity level (level 1), and for F1 sores also for level 2. Evidently, the agreement increases with larger bandwidth, as this means a more generous clustering of time points. Single exception is the normalized DTW distance for level 2. Here, random segmentation and summary segmentation are generally quite close together, particularly, for level 2 of piece 1. 


## Subjective ratings of segment types
In the second trial of the lab version of the experiment, we asked participants to judge at each selected segment position as to what extent (on a scale 1 to 6) they felt that this segment marked a (1) new beginning or (2) an ending. Additionally, we asked them (3) to rate the importance of this boundary.

For analysis, we gathered the gausification peaks with a 2 s bandwidth and selected all boundary marker by participants that could be found in a 2 s window around the peak position. Next, we aggregated all importance, beginning, and ending ratings for each peak as well as the relative peak height (peak height divided by number of raters). 

```{r importance_correlation, fig.cap = "Figure 5. Correlations and distribution over aggregated importance values and relative peak height.", fig.width = 12, fig.height = 8}
annotated_peaks_sum <- find_annotated_peaks(all_boundaries, boundaries_lab_annotations, summary = T)
#annotated_peaks_sum %>% select(rel_peak_height, beginning, ending, importance) %>% correlation::correlation()
# pp_all <- 
#   annotated_peaks_sum %>%  
#   select("Beginning" = beginning, "Ending" = ending, "Importance" = importance, 
#          `Rel. Peak Height`= rel_peak_height) %>%  
#   GGally::ggpairs() + theme_bw() + scale_color_manual(values = "lightblue4")
bind_rows(
  correlation::correlation(annotated_peaks_sum %>%
                             select(-c(piece, peak_pos))) %>%
                             as_tibble() %>%
                             mutate(piece = "both"),
  correlation::correlation(annotated_peaks_sum %>%
                             filter(piece == 1) %>%
                             select(-c(piece, peak_pos))) %>%
                             as_tibble() %>%
                             mutate(piece = "1"),
  correlation::correlation(annotated_peaks_sum %>% filter(piece == 2) %>%
                             select(-c(piece, peak_pos))) %>%
                             as_tibble()%>%
                             mutate(piece = "2")) %>%
  select(piece, x = Parameter1, y = Parameter2, r, p, N = n_Obs) %>%
  mutate(across(where(is.numeric), function(x) round(x,3))) %>% arrange(x,y) -> cor_mat

cor_mat %>%
 filter(p < .05) %>%
 flextable() %>%
 autofit() %>%
 set_caption("Tab. 4. Significant correlation of rater's boundary assessments for both pieces and either piece alone.")
```
We correlated the three assessments with each other and also with the relative peak height, for both pieces together and for each piece alone. Results can be found in Tab. 4, where for simplictiy only the significant correlations are shown.

One see, that the assessment whether something ends or something new begins is in all cases strongly correlated (r about .6, .7). This is not too surprising as this is the definition of a segment. Nonetheless, this was aiming whether the participants were triggered to mark a segment boundary here by a clearly perceived ending or beginning, but it seems this is not easy to do. Perceived importance of a segment correlates with perceived beginning and endings alike, but, interestingly, only for the second piece, which drives the correlation for the combined set. On the other hand, relative peak height, i.e., interrater agreement on segment boundaries only correlates for the first piece. This all indicates that composition specific features are important alike.   


